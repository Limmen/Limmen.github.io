---
title: Expectation Maximization
updated: 2019-05-11 18:30
---

### A Review of Expectation Maximization

Test, $$20000$$,

```python
def em(sample, n_iterations):

    # Initial guess of Theta_0
    mu_1 = np.random.choice(sample, 1)
    mu_2 = np.random.choice(sample, 1)
    std_1 = np.std(sample)/1
    std_2 = np.std(sample)/2
    pi_1 = 1
    pi_2 = 1

    # Iterative estimation
    for it in range(n_iterations):

        # Logging
        if it % 100 == 0:
            print("iteration: {}".format(it))
            print("mu_1: {}, mu_2: {}, std_1:{}, std_2:{}, pi_1: {}, pi_2: {}".format(mu_1, mu_2, std_1, std_2, pi_1, pi_2))


        # E-step: compute the expected log likelihoods for each data point x_i
        pijs = []
        for i in range(len(sample)):
            # Use observed data and guess of Theta_0 to compute (p(z_j | x_i, \theta_0)) for all j and i

            s = 0
            pij_1 = pi_1*(1/(std_1*sqrt(2*np.pi)))*np.exp((-1/2)* pow(((sample[i]-mu_1)/std_1), 2))
            pij_2 = pi_2*(1/(std_2*sqrt(2*np.pi)))*np.exp((-1/2)* pow(((sample[i]-mu_2)/std_2), 2))

            s = pij_1 + pij_2

            pij_1 = pij_1/s
            pij_2 = pij_2/s

            pijs.append([pij_1, pij_2])

        # M-step, maximize theta_1 with respect to the expected log_likelihoods
        # (no differentiation is necessary as maxima is equal to the empirical mean, std, and mixture weight)
        pi_1 = 0
        pi_2 = 0
        mu_1 = 0
        mu_2 = 0

        for i in range(len(sample)):
            pi_1 = pi_1 + pijs[i][0]
            pi_2 = pi_2 + pijs[i][1]
            mu_1 = mu_1 + pijs[i][0]*sample[i]
            mu_2 = mu_2 + pijs[i][1]*sample[i]

        # Arithmethic mean of the sample data given the estimated mixture counts
        mu_1 = mu_1/pi_1
        mu_2 = mu_2/pi_2

        std_1 = 0
        std_2 = 0

        for i in range(len(sample)):
            std_1 = std_1 + pijs[i][0]* pow((sample[i] - mu_1), 2)
            std_2 = std_2 + pijs[i][1]* pow((sample[i] - mu_2), 2)

        # Standard error given the arithmetic means and estimated mixture counts
        std_1 = sqrt(std_1 / pi_1)
        std_2 = sqrt(std_2 / pi_2)

        # Normalize mixture counts into probabilities
        pi_1 = pi_1/len(sample)
        pi_2 = pi_2/len(sample)

    return mu_1, mu_2, std_1, std_2, pi_1, pi_2
```
